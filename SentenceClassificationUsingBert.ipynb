{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentenceClassificationUsingBert.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVxF0IL8+mpVHaB46uaNi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b3cf646c1364664af4b813bf95048b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ffca730a31914119b298da48e1705111",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7fd72b63d704c7bbee2b9f14329134d",
              "IPY_MODEL_7c7c9101c6c44fe696792be4e0e6e58b"
            ]
          }
        },
        "ffca730a31914119b298da48e1705111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7fd72b63d704c7bbee2b9f14329134d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6574d2490b664d679f5a3b3b37978f55",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bea6d328baa413aa40483ee3d6c0bcc"
          }
        },
        "7c7c9101c6c44fe696792be4e0e6e58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e766943f7f44bd59125edd3d2dba39f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 898kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c64341b208d04a19a37b9d46773b746e"
          }
        },
        "6574d2490b664d679f5a3b3b37978f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bea6d328baa413aa40483ee3d6c0bcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e766943f7f44bd59125edd3d2dba39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c64341b208d04a19a37b9d46773b746e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitin649/SentenceClassificationBERT/blob/main/SentenceClassificationUsingBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Efg2EITcEg"
      },
      "source": [
        "import torch \n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy6G067PcIbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f88e81-a4e8-49b3-bab5-8d3eee349ea8"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rQr8qJhcaqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ef8dc0-a7fc-4e5c-a65f-e49984a5e075"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwlutRBiT9aQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ebc56b-31d4-4e93-9db7-ae08b5daf99a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 25.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 3.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 4.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 4.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 378kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 389kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 399kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 409kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 419kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 430kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 440kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 450kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 460kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 471kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 481kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 491kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 501kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 512kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 522kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 532kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 542kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 552kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 563kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 573kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 583kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 593kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 604kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 614kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 624kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 634kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 645kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 655kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 665kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 675kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 686kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 696kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 706kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 716kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 727kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 737kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 747kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 757kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 768kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 778kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 788kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 798kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 808kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 819kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 829kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 839kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 849kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 860kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 870kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 880kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 890kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 901kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 911kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 921kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 931kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 942kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 952kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 962kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 972kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 983kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 993kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.0MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f996fcdbfd53f0bc2f6042f14cc1bc757b3889a7c08661cb1f86ba1143066877\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju-NtE6BUXD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d5c1a2-1f03-41be-b621-636b61ef4ec3"
      },
      "source": [
        "#loading dataset\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=a8ece0c1b69b2792b6783ca515ce636a17b0e9f3ffeaf58563522c413cce0a20\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Pybi6BUnC5"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "url='https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'#this url is used for downloading the dataset\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJg1Q9NRU5jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f84cff-dd67-40f6-e671-082e1b843e16"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5CW2B2sU9HY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "8de50558-e779-4ea9-f5ef-1c76cde76f32"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7932</th>\n",
              "      <td>ad03</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Has not the potion worked</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5070</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I believe strongly that the world is round.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2108</th>\n",
              "      <td>rhl07</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Robin arrived partway at the station.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2621</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Doug removed the tabletop of scratches.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8103</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For you to do that would be a mistake.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4400</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fred must have been both singing songs and dri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5469</th>\n",
              "      <td>b_73</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>She is as brilliant a woman as her mother.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6668</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The terrier attacked the burglar and savaged t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8500</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What Alison and David did was soak their feet ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3569</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John not leave.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "7932            ad03  ...                          Has not the potion worked\n",
              "5070            ks08  ...        I believe strongly that the world is round.\n",
              "2108           rhl07  ...              Robin arrived partway at the station.\n",
              "2621            l-93  ...            Doug removed the tabletop of scratches.\n",
              "8103            ad03  ...             For you to do that would be a mistake.\n",
              "4400            ks08  ...  Fred must have been both singing songs and dri...\n",
              "5469            b_73  ...         She is as brilliant a woman as her mother.\n",
              "6668            m_02  ...  The terrier attacked the burglar and savaged t...\n",
              "8500            ad03  ...  What Alison and David did was soak their feet ...\n",
              "3569            ks08  ...                                    John not leave.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPlKJZ-DVF9D"
      },
      "source": [
        "#2 main features we need from this data first one is sentence and second one is label which decides whether the particular sentence is + or -\n",
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikfXmmJzVddm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9b3cf646c1364664af4b813bf95048b2",
            "ffca730a31914119b298da48e1705111",
            "c7fd72b63d704c7bbee2b9f14329134d",
            "7c7c9101c6c44fe696792be4e0e6e58b",
            "6574d2490b664d679f5a3b3b37978f55",
            "2bea6d328baa413aa40483ee3d6c0bcc",
            "4e766943f7f44bd59125edd3d2dba39f",
            "c64341b208d04a19a37b9d46773b746e"
          ]
        },
        "outputId": "0ce71089-6f56-439e-e751-22186ee4dfda"
      },
      "source": [
        "#tokenize and input formatting of our data\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b3cf646c1364664af4b813bf95048b2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99wvR5_zVq38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3247d5f-aff5-435c-d2fc-d4b017c75ac2"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g40C-xbbVvn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2a3a33-b787-478d-a63f-ed44de593d3d"
      },
      "source": [
        "#required formatting \n",
        "  #Add special tokens to the start and end of each sentence.\n",
        "  #Pad & truncate all sentences to a single constant length.\n",
        "  #Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
        "\n",
        "#tokenize data and check the maximum len we get\n",
        "max_len=0\n",
        "\n",
        "for sent in sentences:\n",
        "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    \n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print('max length',max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max length 47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRHvcIXXz09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc1af60-aec1-450a-b2af-e22b3765bba5"
      },
      "source": [
        "#the tokenizer.encode_plus function combines multiple steps for us:\n",
        "\n",
        "#Split the sentence into tokens.\n",
        "#Add the special [CLS] and [SEP] tokens.\n",
        "#Map the tokens to their IDs.\n",
        "#Pad or truncate all sentences to the same length.\n",
        "#Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n",
        "#The first four features are in tokenizer.encode, but I'm using tokenizer.encode_plus to get the fifth item (attention masks).\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    #encoded_dict have 3 components input ids  , mask_ids and segment ids as well\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
            "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqCvqdvwY7qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bc600a-6fab-4a48-8ef6-4649f299c714"
      },
      "source": [
        "#training and validation split\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz9Ht3pkZca3"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFPqfUmHb_Uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b18b11-907c-47e1-eaa8-8253e47b9654"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ").to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ecPm3akdgHF"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opPqtk05d4TS"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHCy3JQed6gv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR_vmbcxeSTz"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8wdOrIEec13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc725a82-89f8-4487-e073-ba5f801413c7"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        #print(device)\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        #print(b_labels,b_input_ids,b_input_mask)\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "       \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:55.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:08.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:21.\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:01:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "  Validation Loss: 0.92\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:54.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:07.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:21.\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:01:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "  Validation Loss: 0.92\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:54.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:07.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:21.\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:01:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "  Validation Loss: 0.92\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:54.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:07.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:20.\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:01:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "  Validation Loss: 0.92\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:05:38 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGqT9fgNe4Ft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "cf29b0e1-5fd4-4ead-e376-64dd89a12209"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0:01:22</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0:01:21</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0:01:21</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0:01:21</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.88         0.92           0.27       0:01:22         0:00:03\n",
              "2               0.88         0.92           0.27       0:01:21         0:00:03\n",
              "3               0.88         0.92           0.27       0:01:21         0:00:03\n",
              "4               0.88         0.92           0.27       0:01:21         0:00:03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3g0218Mz-nn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}